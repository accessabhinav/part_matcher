{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of documents: 2 \n",
      "\n",
      "\n",
      " LHD control mounting reinforcement deletion from LHD BIW .\n",
      "[('lhd control mounting reinforcement deletion from rhd biw .', 93), ('deletion of front roof bow frm roof structure .', 51)]\n",
      "\n",
      " Add sunroof cover mechanism .\n",
      "[('sunroof cover mechanism deletion .', 78), ('rear wall inner panel deletion with addition of window level cross member .', 32)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def listtostring(clean_line):\n",
    "    str1 = \" \"\n",
    "    return(str1.join(clean_line))\n",
    "\n",
    "#using Lemmatization\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "file_docs = []\n",
    "test_doc = []\n",
    "test_line = []\n",
    "\n",
    "with open ('E:\\JS\\Py\\Text_similarity\\Idea_List.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    \n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "        \n",
    "# create a clean document after removing the stop-words and generate lemmas\n",
    "\n",
    "clean_doc = []\n",
    "for line in file_docs:\n",
    "    clean_line = [word for word in line.split() if word not in stopwords]\n",
    "    \n",
    "    for word in line.split():\n",
    "        test_line.append(lemma.lemmatize(word))\n",
    "    test_doc.append(listtostring(test_line))\n",
    "    test_line = []\n",
    "    \n",
    "    #clean_doc.append(listtostring(clean_line))\n",
    "    clean_doc = test_doc\n",
    "\n",
    "#print(\"Number of documents:\",len(clean_doc))\n",
    "\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in clean_doc]\n",
    "\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "#for line in clean_doc:\n",
    "    #print(line)\n",
    "\n",
    "# building the index\n",
    "sims = gensim.similarities.Similarity('E:/JS/Py/Text_similarity/',tf_idf[corpus],num_features=len(dictionary))\n",
    "\n",
    "file2_docs = []\n",
    "\n",
    "with open ('E:\\JS\\Py\\Text_similarity\\Idea_Query.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "        \n",
    "test_doc = []\n",
    "clean_doc2 = []\n",
    "for line in file2_docs:\n",
    "    clean_line = [word for word in line.split() if word not in stopwords]\n",
    "    \n",
    "    for word in line.split():\n",
    "        test_line.append(lemma.lemmatize(word))\n",
    "    test_doc.append(listtostring(test_line))\n",
    "    test_line = []\n",
    "    \n",
    "    clean_doc2 = test_doc\n",
    "\n",
    "print(\"\\nNumber of documents:\",len(clean_doc2),'\\n')\n",
    "\n",
    "for line in clean_doc2:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and create bag of words\n",
    "    \n",
    "    sims = gensim.similarities.Similarity('E:/JS/Py/Text_similarity/',tf_idf[corpus],num_features=len(dictionary))\n",
    "\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "    \n",
    "new_doc = [line.lower() for line in clean_doc]\n",
    "\n",
    "limit = 2\n",
    "\n",
    "for line in clean_doc2:\n",
    "    sim_score = process.extract(line.lower(), new_doc, scorer=fuzz.token_sort_ratio, limit = limit)\n",
    "    print(\"\\n\",line)\n",
    "    print([line for line in sim_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
